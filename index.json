[{"authors":[],"categories":[],"content":" There have been many advances in the field of Machine Learning in the recent years, but considering how data-intensive this field is (especially deep learning), most of these advances will fall short if the back-end infrastructure is not robust enough to handle large scale data. While increase in compute power has helped in the advancement of this field, an equal amount of innovation has been necessary in how the data is managed. The database community has been actively working on tackling data management-related challenges that arise in large-scale machine learning systems.\nIn this post I will be covering some of the recent advances that have been made with regards to ML-oriented database systems, and end with some open problems which still lack good solutions. This post is mainly based on this paper by Kumar et. al. This post is intended to serve as a reference guide for the interested readers to exlpore various popular systems that have been developed to serve the needs of large scale ML systems. I will be linking relevant source pages and research papers for the systems that I mention throughout this post.\nEver since the data mining boom of the late 1990s and early 2000s, the database community has been working on data management-oriented challenges in ML. A number of systems have therefore been developed for scalable and fast ML, some of which I will be referring to in this article.\nThe development in the ML-oriented DB systems is mainly oriented along three lines of work:\n Integrating ML algorithms and languages with existing data systems such as RDBMSs Adapting data management-inspired techniques such as query optimisation Combining data management and ML ideas to build systems that improve ML lifecycle-related tasks  ML in Data Systems ML computations can be integrated with an RDBMS to bring it closer to where the data resides. This avoids the cost of having to move the data to specialised ML toolkits. Below listed are some of the methods used to achieve this:\n Earlier scalable ML techniques exploited user-defined function (UDF) and user-defined aggregate (UDA) abstractions in data systems. Examples of these kind of systems are ATLAS, in-RDBMS ML libraries such as Oracle Data Mining and GLADE.  ATLAS - DBMSs have long suffered from SQL’s lack of power and extensibility. ATLAS is a powerful database language and system that enables users to develop complete data-intensive applications in SQL by writing new aggregates and table functions in SQL rather than in procedural languages. GLADE - GLADE is a scalable distributed framework for large scale data analytics. GLADE arised out of the need to serve the requirements of companies that wanted to apply statistics and machine learning methods on data, since SQL and other relational DB systems do not support it directly.  Efforts have also been made to integrate ML with data systems by optimising ML over datasets that are logically the output relational queries, especially, joins. Orion is one such example, which introduced \u0026ldquo;factorised learning\u0026rdquo; to push generalised linear models through joins to avoid redundancy in ML computations. RDBMSs can also be used to support complex mutli-relational ML models known as statistical relational learning. DeepDive is an example of such a system, which exploits the advanced join processing capabilities of RDBMSs to scale inference systems which makes it possible to apply such methods to large-scale databases. To help the users focus more on the learning task at hand, some systems have been developed to provide higher levels of abstractions to simplify the development and customisation of ML algorithms. These systems work by either generating SQL queries or generating jobs for data-parallel frameworks such as Spark or Hadoop. An example of such a system is Riot-DB.  DB-INSPIRED ML SYSTEMS This section covers systems and domain-specific languages (DSLs) inspired by databases, programming languages and high-performance computing.\n A number of state-of-the-art optimising compilers for ML algorithms like SystemML, OptiML, rely on simplification rewrites. Modern in-memory database systems often apply query compilation. SystemML introduced a holistic framework for automatic rewrite identification and operator fusion, including the generation of sparsity-exploiting operators. Since manly ML algorithms are iterative and perform repeated matrix-vector multiplications, systems like SystemML employ lightweight database compression techniques and execute linear algebra operations directly on the compressed matrix representation.  ML LIFECYCLE SYSTEMS This section covers advances in tasks such as model selection and model management, which apply ideas which are databases oriented, such as declarativity, interactivity, and optimisation.\n Feature Engineering - Feature engineering is often the most time-consuming part of an applied ML project. Advances in this domain involve using database-inspired ideas such as indexing and partitioning to read only parts of the data so that the access times are reduced. Zombie is one such implementation. KeystoneML provides libraries for certain forms of featurisation and optimises pipelines of ML operators over Spark. Hamlet applies statistical learning theory to exploit database dependencies to drop features before using the data for learning the model without significantly affecting the accuracy. Model Selection and Management - It is the process of obtaining the most appropriate machine learning model with respect to the learning task at hand. There have been a couple of attempts to automate this process. Longview integrates model management into a DBMS to automate certain aspects of model selection, ModelHub proposed a language and storage manager for managing deep NNs which are common in computer vision, and ModelDB instruments ML libraries to capture and manage models. There are several cloud ML services as well, such as Mircosoft’s AzureML which aims to simplify and manage end-to-end ML workflows.  Open Problems  Size and Sparsity estimation: Many optimization techniques require prior knowledge of the size and sparsity of matrices for cost comparisons and valid plan generation. However, it turns out that this task is non-trivial for programs to infer in complex methods involving linear algebra. Therefore, principled techniques are required for estimating the size and sparsity of matrices in data. Convergence estimation: Most ML algorithms involve finding the most optimal solution in an iterative fashion, in which the algorithm tries to converge to the optimal solution over a number of steps. Knowing the number of steps that it would take for the algorithm to converge would help in better resource allocation, and for optimising data re-organisations. It would also help in estimating the amount of progress that has been made towards reaching the optimal solution. Adaptive Query processing and storage: Generally, in case the workloads changes frequently, adaptive query processing and storage techniques are used to handle the workload. Integrating Relational and Linear Algebra: A seamless integration of relational and linear algebra is required so that users can easily perform data transformations for feature engineering such as joins and aggregates, and training and prediction of different ML models. ML system benchmarks: Benchmarks that compare the implementation of very large scale machine learning systems would be very helpful. There already exist some benchmarks that try to address this, such as this paper by Cai et. al. However, even this paper only uses four different platforms such as Spark and SimSQL. A more comprehensive survey would be highly beneficial.  ","date":1553503070,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553503070,"objectID":"b1bd2433a4f71eaaeb23bc1a0cb45438","permalink":"https://ritik99.github.io/post/data/","publishdate":"2019-03-25T14:07:50+05:30","relpermalink":"/post/data/","section":"post","summary":"There have been many advances in the field of Machine Learning in the recent years, but considering how data-intensive this field is (especially deep learning), most of these advances will fall short if the back-end infrastructure is not robust enough to handle large scale data. While increase in compute power has helped in the advancement of this field, an equal amount of innovation has been necessary in how the data is managed.","tags":[],"title":"Data Management in Machine Learning","type":"post"},{"authors":null,"categories":[],"content":"","date":1541827176,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541827176,"objectID":"93240b7642fb3fa64ff6b4e3117f4ee3","permalink":"https://ritik99.github.io/project/image-panaroma/","publishdate":"2018-11-10T10:49:36+05:30","relpermalink":"/project/image-panaroma/","section":"project","summary":"","tags":[],"title":"Image Panaroma","type":"project"},{"authors":null,"categories":[],"content":"","date":1541827128,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541827128,"objectID":"acbb927ca8afcd8068457e28bb9f01d0","permalink":"https://ritik99.github.io/project/denoising-images/","publishdate":"2018-11-10T10:48:48+05:30","relpermalink":"/project/denoising-images/","section":"project","summary":"","tags":[],"title":"Denoising Images","type":"project"},{"authors":null,"categories":[],"content":"","date":1541827097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541827097,"objectID":"41473f556e56505406e610aceeea9449","permalink":"https://ritik99.github.io/project/eigenfaces/","publishdate":"2018-11-10T10:48:17+05:30","relpermalink":"/project/eigenfaces/","section":"project","summary":"","tags":[],"title":"Eigenfaces","type":"project"},{"authors":null,"categories":[],"content":"","date":1541827064,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541827064,"objectID":"219d4e2f860b61de9c79c2a293aa44a4","permalink":"https://ritik99.github.io/project/insult-detection/","publishdate":"2018-11-10T10:47:44+05:30","relpermalink":"/project/insult-detection/","section":"project","summary":"","tags":[],"title":"Insult Detection","type":"project"},{"authors":null,"categories":[],"content":"Natural Language Generation (NLG) is characterised as \u0026ldquo;the sub-field of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable tests in English or other human languages from some underlying non-linguistic representation of information\u0026rdquo; (Reiter \u0026amp; Dale, 1997). NLG is one of the earliest topics that researchers started taking interest in.\nFollowing are the most frequent applications of NLG systems:\n Content determination: Determining what kind of information to include while preparing a document Text structuring: Determining the sequence in which information is supposed to appear Sentence aggregation: Deciding information which should be displayed together Lexicalisation: Finding the right words and phrases to express information Referring expression generation: Selecting the words and phrases to identify domain objects Linguistic realisation: Combining all words and phrases into well-formed sentences  All of these topics are fairly complex, and most applications usually involve a combination or all of the above mentioned areas.\nSo, in this blogpost I\u0026rsquo;ll be talking about generating puns using Natural Language Generation which involves some parts of all the above stated applications.\nGenerating puns falls under the field known as computational humor, which is a branch of computational linguistics focused on humor research (there have also been dedicated conferences for this field!).\nFor those of you who don\u0026rsquo;t know what puns are, here are a few punny ones-\nI was going to make myself a belt made out of watches, but then I realized it would be a waist of time.\nWhat do you get when you cross a murderer with a breakfast food? A cereal killer.\nAs illustrated in the above examples, making puns involves using the fact that there are different meanings for a given word or that there are words that sound similar but hold different meanings.\nWhile currently there are a couple of innovative and more truly \u0026ldquo;natural\u0026rdquo; methods for generating puns, the most basic and popular method for generating sentences (and puns) is using a template approach, in which the main context of the sentence is set, while the fillers are derived by the generation model being used. For example, a template might be of the following form: \u0026ldquo;The population of the [country] is [number]\u0026ldquo;, where [country] and [number] are filled in by the model at the run-time. It seems tedious and a little less galmorous, but a large part of the amazing results we see or hear are based on such rule-based methods.\nOne of the most popular models developed in this field is JAPE (Joke Analysis and Production Engine). JAPE is designed to output question-and-answer type puns from a general lexicon. While seemingly easy at first, the model gets fairly complex, with a number of rules being used for coming up with a valid question and sound answer. An example output of this program is:\nQ. What do you call a cry that has pixels? A. A computer scream\nThe original paper on this model by Kim Binsted and Graeme Ritchie can be found here.\nFor the sake of this post, I will be only considering the case of being able to generate words that sound similar to the an input sentence, while still making sure that the sentence makes sense. This is the most basic method, and can be built upon to make more complex mechanisms.\nThe actual idea is pretty simple and the code is similar to the one used by Max Schwartz for his talk at PyGotham 2017, which I\u0026rsquo;ve linked below. I take in a sentence as an input, and barring the stop words, for every other word, I look through a list for words with similar pronounciation and the same POS tag, and then replace it with the best fit. Only those words are considered whose edit distance from the word that is being replaced is below a threshold. Varying this threshold changes how similar the new sentence sounds to the old one.\nThe next step was to generate a list containing the pronounciation and the most likely POS tag for each word in our dictionary. This was done using ARPABET, which is a pronouncing dictionary managed by CMU. Storing the POS tag helps in picking up words that similar sounding to the word we try to replace.\nAnd that is pretty much it. With a few lines of code, I was happy with the results I was getting. An example:\nInput: I am a serial killer.\nOutput: I am a cereal tiller.\nThe implementation was pretty simple, and hence the output is, for most of the time, not that great. Of course, increasing the complexity can increase the kind of outputs we get.\nThe iPython notebook accompanying this post can be found here.\nReferences:\n Building a \u0026ldquo;Pun Generator\u0026rdquo; in Python - https://www.youtube.com/watch?v=6gJKxe5zPXM PyGotham Talks - https://github.com/M0nica/PyGotham-2017-Talks An implementation - https://github.com/maxwell-schwartz/PUNchlineGenerator Computational Humor Seminar - https://www.cse.iitb.ac.in/~vipulsingh10/me/ComputationalHumourSeminar.pdf  ","date":1541826154,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541826154,"objectID":"36f14950d131e58915ef954a1b14eef8","permalink":"https://ritik99.github.io/post/nlg/","publishdate":"2018-11-10T10:32:34+05:30","relpermalink":"/post/nlg/","section":"post","summary":"Natural Language Generation (NLG) is characterised as \u0026ldquo;the sub-field of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable tests in English or other human languages from some underlying non-linguistic representation of information\u0026rdquo; (Reiter \u0026amp; Dale, 1997). NLG is one of the earliest topics that researchers started taking interest in.\nFollowing are the most frequent applications of NLG systems:\n Content determination: Determining what kind of information to include while preparing a document Text structuring: Determining the sequence in which information is supposed to appear Sentence aggregation: Deciding information which should be displayed together Lexicalisation: Finding the right words and phrases to express information Referring expression generation: Selecting the words and phrases to identify domain objects Linguistic realisation: Combining all words and phrases into well-formed sentences  All of these topics are fairly complex, and most applications usually involve a combination or all of the above mentioned areas.","tags":[],"title":"Natural Language Generation - Generating Puns","type":"post"}]